#+TITLE: MapReduce Implementations of Particle Filters with Distributed Resampling Schemes
#+AUTHOR: Erli Ding
#+OPTIONS: H:3 toc:nil
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper]
#+LATEX_HEADER: \usepackage{amsmath} \usepackage{algorithm} \usepackage{algorithmic}

#+BEGIN_ABSTRACT
This paper explores the possibilities of parallelizing particle filtering
algorithms as MapReduce tasks in Hadoop. A novel resampling scheme for parallel
implementation of particle filter is proposed. The proposed algorithm utilize a
partilce redistribution mechanism to completely eliminate the global collective
operation, such as global weight summation or nomalization, in resamling phase
of particle filtering algorithms. This scheme divides sub-populations of
particles in each Map task into sub-groups and introduces a uniform sampling
step to regroup sub-populations received by each Reduce task. This algorithm
achieves a fully distributed implementation of particle filters while keeps the
estimation unbiased. Our simulation uses the Hadoop streaming interface for
MapReduce implementation on various number of computation nodes. Simulation
results show that the MapReduce implementation can achieve a significant speedup
over the serial particle filter and the proposed resampling scheme outperforms
the others in execution time while has no performance loss in estimation
accuracy. Compared to other resampling schemes, the proposed scheme can also be
adapted to MapReduce programming model more naturally, one job per iteration
with multiple number of both Map tasks and Reduce Tasks.
#+END_ABSTRACT

*Keywords*: sequential Monte Carlo, particle filter, distributed resampling, MapReduce

* Introduction

Sequential Monte Carlo (SMC) methods, also known as particle filters (PF), are a
class of recursive Bayesian filters based on Monte Carlo simulation for state
estimation of nonlinear and/or non-Gaussian state space models. Because the
precision of estimation depends only on the number of particles used in
approximating the distribution, a large number of particles have to be evaluated
at each filter iteration in order to achieve satisfactory accuracy, which makes
particle filtering a computationally intensive method. Hence the development of
parallel or distributed processing schemes to reduce high computational cost is
essential for effective implementations of PFs in practice. In general, particle
filtering algorithms consist of two major stages: propagation stage and
resampling stage. During propagation stage, particles are first generated in the
sampling step and then can be used for weight update. Since there are no
dependencies between operations on each particle and its weight, computations in
this stage can be embarrassingly parallelized. However this independence can
lead to high variance, particle degenerancy problem, in the resulting
estimation. The introduction of resampling stage aims to reduce the effects of
the degenerancy problem. But resampling also limits the opportunity to
parallelize since all the particles must be combined in this stage for joint
processing, such as weight summation and normalization.

While parallel processing techniques can effectively solve time-consuming
simulation problems, practical considerations can be an obstacle to porting
existing single-threaded codes onto parallel architectures. To utilize parallel
resources efficiently, programmers must be skilled in parallel programming and
spend substantial effort optimizing the parallel portion of their code. Parallel
code is also harder to debug and maintain. Last, large computer clusters are not
always available for running massively parallel applications. All these
practical issues are important drawbacks to the development and use of
distributed algorithms.

MapReduce was developed at Google as a new framework to facilitate the
development of parallel algorithms. MapReduce is a domain-independent
programming model for processing data in a highly parallel fashion. With
MapReduce, parallel computing can be automatically carried out in large-scale
commodity machines. MapReduce can hide the complexity of parallelization, data
distribution, fault-tolerance, and load balancing to the developer, which can
focus on developing the actual algorithm. In this programming model, the
developer specifies simple tasks which are applied in a distributed fashion to
large datasets.Applications such as data mining, image processing, and pattern
recognition have successfully used MapReduce to solve computationally
challenging problems.

While it is clear that Hadoop provides many advantages for creating distributed
applications, the MapReduce model is mainly designed for relatively simple data
processing algorithms which do not require much data and task synchronization
and are generally characterized as embarrassingly parallel algorithms. One
MapReduce job consists of a single Map and a single Reduce operation and the
synchronization is only performed between these two operations, which makes
adapting complex scientific computing algorithms to MapReduce a non-trivial
task. The structure of a MapReduce application is very strict. It is not trivial
to reduce complex algorithms to the MapReduce model and there is no guarantee
that the resulting MapReduce algorithms are effective.

In this paper, we propose a new distributed resampling scheme for parallel
implementation of PFs. Compared to existing resampling scheme, this new
resampling scheme has no need of employing a master node for centralized
processing. In this sense, it is totally distributed while maintain the same
performance as those scheme containning more or less centralized processing
stage. We also evaluate the benefits of MapReduce technology in the context of
particle filtering methods. We utilizes the parallel and distributed processing
capability of Hadoop MapReduce and demonstarte that MapReduce is also well
suited for computationally damanding Monte Carlo simulations. We parallelize the
sampling and weight computation steps based on the MapReduce programming model
and handle the weight normalization and resampling steps in Hadoop Distributed
File System (HDFS). Our proposed resampling scheme has a more straightforward
interpretation for the MapReduce programming model.

* Particle Filters

Consider the following general state space or hidden Markov model, which consists
of two equations: an observation (or mesurement) equation and a transition(or
system) equation
\begin{align}
  \label{eq:obmodel}
  y_n &= h(x_n,v_n),\\
  \label{eq:statemodel}
  x_n &= f(x_{n-1},u_n).
\end{align}
The observations y_n and state variables x_n may be continuous-valued,
discrete-valued or a combination of the two. {v_n} and {u_n} are
independent sequences of i.i.d. random variables, respectively. The densities
p(y_n | x_n) and p(x_n | x_{n-1}) corresponding to \ref{eq:obmodel} and
\ref{eq:statemodel} are called observation density and transition density.
The collection of observable variables up to time n is denoted by y^n.

Roughly speaking, the estimation is performed by approximating the posterior
distribution of the latent state variables at each time step given measurements
up to that time step, p(x_n | y^n), by a set of i.i.d. particles
{x_n^{(k)}}_{k=1}^N and associated importance weights {w_n^{(k)}}_{k=1}^N,
where the weights sum to one \sum_{k=1}^N{w_n^{(k)}}=1. This relation is
conventionally denoted as {x_n^{(k)}, w_n^{(k)}}_{k=1}^N \sim p(x_n | y^n).
Given a set of particles and weights, the posterior mean of any function of the
state variable g(x_n) can be estimated by Monte Carlo integration
\begin{displaymath}
  E[g(x_n) \mid y^n] \approx \sum_{k=1}^N{g(x_n^k)w_n^k}.
\end{displaymath}

In general, most particle filters involve a two-step procedure: propagation
step and resampling step. The propagation step deals with how to move
particles forward from {x_{n-1}^{(k)}, w_{n-1}^{(k)}}_{k=1}^N to
{x_n^{(k)}, w_n^{(k)}}_{k=1}^N, which consists of two steps. The prediction step
\begin{displaymath}
  p(x_n \mid y^{n-1})=\int{\frac{p(x_n\mid x_{n-1})}{q(x_n \mid x_{n-1},y^n)}q(
    x_n \mid x_{n-1},y^n)p(x_{n-1} \mid y^{n-1}) \, dx_{n-1}},
\end{displaymath}
where q(x_n | x_{n-1}, y^n) is the importance distribution to be chosen and
\( \frac{p(x_n \mid x_{n-1})}{q(x_n | x_{n-1}, y^n)} \), is termed the unnormalized
importance weight. And the update step
\begin{displaymath}
  p(x_n \mid y^n) \propto p(y_n \mid x_n)p(x_n \mid y^{n-1})
\end{displaymath}
is derived by Bayes rule.

The propagation step is often vulnerable to the weight degeneracy problem,
that is, a large subset of the particles is assigned very small weights and
hence the effective size of particles is reduced, which leads to greater
approximation errors. An additional resampling step is therefore introduced to
mitigate the weight degeneracy problem by replicating a new population of
particles from the existing population in proportion to their importance weights.
Thus particles with large weights are randomly duplicated while particles with
small weights are removed. The basic resampling algorithms are multinomial,
residual, stratified and systematic resampling. They are all unbiased, but
differ in terms of variance.

The bootstrap filter (BF) introduced in the seminal paper (\cite{ref:njg93})
uses the transition density function as importance distribution.
It is a type of propagate-resample filter based on the representation
\begin{align}
  \label{eq:bf}
  p(x_n,x_{n-1} \mid y_n,y^{n-1})&\propto p(y_n \mid x_n)p(x_n \mid x_{n-1})p(
  x_{n-1} \mid y^{n-1}),
\end{align}
which can be summarized in the following two steps:
1. Propagation: Particle set \( \{\tilde{x}_n^{(k)}, \tilde{w}_n^{(k)}\}_{k=1}^N \sim p(x_n \mid y^n) \)
   is sampled from \( p(x_n | x^k_{n-1}) \) where weight
   \( \tilde{w}_n^k \propto p(y_n \mid \tilde{x}_n^k) \) for k=1, 2, ..., N.
2. Resampling: Resample \( {x_n^{(k)}}_{k=1}^N \) from the propagated particle set
   \( \{\tilde{x}_n^{(k)}, \tilde{w}_n^{(k)}\}_{k=1}^N \) for k=1, 2, ..., N.

The auxiliary particle filter (APF) developed in Pitt and Shephard (1999) is a
type of resample-propagate filter which significantly reduces weight degeneracy
problems by taking into account the next observation and giving more importance
to particles with large predictive values. The identity from representation
(\ref{eq:bf}) can be rewritten as
\begin{align}
  \label{eq:apf}
  p(x_n,x_{n-1} \mid y_n,y^{n-1}) &\propto p(x_n \mid x_{n-1},y^n)p(
    y_n \mid x_{n-1})p(x_{n-1} \mid y^{n-1})\\
  &=\frac{p(y_n\mid x_n)}{p(y_n\mid \mu(x_{n-1}))}p(x_n\mid x_{n-1})p(
    y_n \mid \mu(x_{n-1}))p(x_{n-1} \mid y^{n-1}),
\end{align}
which can be summarized in the following two steps:
1. Resampling: Resample \( \{\tilde{x}_{n-1}^{(k)}\}_{k=1}^N \) from particle set
   \( \{x_{n-1}^{(k)}, p(y_n \mid \mu(x_{n-1}^{(k)}))w_{n-1}^{(k)}\}_{k=1}^N \) for
   k=1, 2, ..., N, where \( p(y_n \mid \mu(x_{n-1})) \) is the observation density
   \( p(y_n\mid x_n) \) evaluated at \( \mu(x_{n-1}) \) (usually the mean, median,
   mode or random probe of the transition density \( p(x_n\mid x_{n-1}) \)).
2. Propagation: Particle set \( \{x_n^{(k)}, w_n^{(k)}\}_{k=1}^N \sim p(x_n \mid y^n) \)
   is sampled from \( p(x_n \mid \tilde{x}_{n-1}^k) \) draw \( x_n^k \) where weight
   \( w_n^k \propto \frac{p(y_n \mid x_n^k)}{p(y_n \mid \mu(\tilde{x}_{(n-1)}^k))} \)
   for k=1, 2, ..., N.

* Resampling Schemes

The major issue in distributing computations of PFs over different compute nodes
comes from the development of parallel resampling schemes which can avoid
centralized processing of particles across different to some degree. In this
section, we discuss different distributed resampling schemes and also present
a new scheme which is fully distributed without the need for a master node. We
present methods for bootstrap filter only, but these methods can be easily
adapted to apply for APF or other type of partilce filters.

** Partially Distributed Resampling

Let /N/ be the total number of particles, /N_m^i/ be the number of particles in
/i/ th compute node /m_i/ and /M/ be the number of compute nodes
\( \sum_{i=1}^M{N_m^i}=N \). Particle set \( \{x^{(k)}, w^{(k)}\}_{k=1}^N \) is
partitioned into $M$ sub-populations, \( \{x^{i,(j)}, w^{i,(j)}\}_{j=1}^{N_m^i} \)
where \( i=1,2,\ldots,M \), which are then distributed onto /M/ compute nodes.

1. Centralized resampling
   This is the easist way to implement a distributed particle filter. The
   propagation step of each sub-population is calculated parallely at multiple
   compute nodes while a global resampling step is performed serially at a single
   master node for all particles.
   - Perform propagation step on each compute node $m_1,m_2,\ldots,m_M$ in
     parallel and send propagated sub-populations
     \( \{\tilde{x}^{i,(j)}, \tilde{w}^{i,(j)}\}_{j=1}^{N_m^i} \) to the master node.
   - Normalize weight and perform resampling step on propagated particle
     set \( \{\tilde{x}^{(k)}, \tilde{w}^{(k)}\}_{k=1}^{N} \).
2. Distributed resampling with centralized weight normalization\\
   In this implementation of distributed PFs, both the propagation step the
   resampling step are performed locally at the compute node where sub-population
   of particles is located. In the end, an additional global normalization step is
   caculated at the master node to reweight all the resampled particles. Without
   this step the global estimation result would be biased though local estimation
   on each compute node is still unbiased.
   - On each compute node, propagate particles, evaluate the sum of local
     weights \( \tilde{W}_m^i=\sum_{j=1}^{N_m^i}{\tilde{w}^{i,j}} \) where
     $i=1,2,\ldots,M$, resample particles, and send resampled sub-populations
     along with the sum of local weight to the master node.
   - Calculate the sum of global weights
     \( \tilde{W}=\sum_{i=1}^M{\tilde{W}_m^i} \) and reweight all the particles by
     the ratio of their associated local weight sum to the global weight sum.
3. Distributed resampling with centralized sampling of resampling number\\
   While this resampling scheme also performs propagation and resampling steps
   locally at each compute node, it adds an additional centralized sampling step
   in between. This extra step draws a sample of resampling number of particles
   for each compute node. Because of this step, all resampled particles are equally
   weighted and the estimation keeps unbiased.
   - Propagate particles in parallel at each compute node, evaluate the
     sum of local weights \( \tilde{W}_m^i$ and send \( \tilde{W}_m^i \) to the
     master node, where $i=1,2,\ldots,M$.
   - At the master node, sample resampling numbers
     \( \{\tilde{N}_m^{(i)}\}_{i=1}^M \) multinomially with weight proportional to
     \( \{\tilde{W}_m^{(i)}\}_{i=1}^M \), where\( \sum_{i=1}^M{\tilde{N}_m^i}=N \).
     Send resampling numbers to their corresponding compute node.
   - On each compute node, perform resampling step in parallel.
4. Distributed resampling with centralized sampling of resampling node\\
   This resampling scheme is similar to the above one except that the additional
   sampling step here, instead of sampling resampling number, samples the
   replication number of each compute node. The resampling step is then performed
   one or multiple times on each node according to the repication number being
   sampled. If the replication number is zero, the sub-population on the
   corresponding node is discarded.

** Fully Distributed Resampling

Since all the above resampling schemes contain a centralized step they are not
fully distributed. The extra centralized step provides an interaction between
all sub-populations. With the introduction of the interaction step, the global
estimation of posterior distribution can be unbiased. To achieve certain level
of interaction between sub-populations on different compute node, the resampling
scheme proposed in this paper propose a redistribution step which can make the
algorithm fully distributed while keep the estimation unbiased.

- Fully distributed resampling with uniform sampling and particle regrouping\\
  Partition particles into $M$ sub-populations each containing $N_m^i$,
  $i=1,2,\ldots,M$ particles and distribute these sub-populations on $M$ compute
  nodes. After performing in parallel the propagation step at each compute node,
  sample uniformly \( \bar{N}_m^i=M \lceil\frac{N_m^i}{M}\rceil \) number of particles
  from propagated particle set \( \{\tilde{x}^{i,(j)},\tilde{w}^{i,(j)}\}_{j=1}^{N_m^i} \),
  $i=1,2,\ldots,M$. Thus, each uniformly sampled sub-population
  \( \{\bar{x}^{i,(j)},\bar{w}^{i,(j)}\}_{j=1}^{\bar{N}_m^i} \) has the expected value
  of weight sum \( \bar{W}_m^i= \frac{\bar{N}_m^i}{N_m^i} \tilde{W}_m^i \),
  $i=1,2,\ldots,M$. Divide each sampled sub-population into $M$ sub-groups each
  containing \( \lceil \frac{N_m^i}{M} \rceil \) particles. Then the expected value
  of weight sum in each sub-group is $\frac{\bar{W}_m^i}{M}$, $i=1,2,\ldots,M$.
  In the redistribution step, pick one sub-group from each compute node, merge
  them into a new sub-population $\{\hat{x}^{i,(j)},\hat{w}^{i,(j)}\}_{j=1}^{\hat{N}_m}$
  and send it to a compute node. Now each redistributed sub-population has the
  same number of particles \( \hat{N}_m=\sum_{i=1}^M{\lceil\frac{N_m^i}{M}\rceil} \)
  and the same expected value of weight sum \( E[\hat{W}_m]=\sum_{i=1}^M{\frac{
  \bar{W}_m^i}{M}} \). In the final step, resampling in parallel $N_m$ particles
  from each redistributed sub-population, where $N_m$ is the avarage number of
  particles at each compute node $MN_m=N$. After the this resampling step, all the
  resampled particles are equally weighted.
  - At each compute node, propagate particles and sample uniformly
    \( M \lceil\frac{N_m^i}{M}\rceil \) number of particles from propagated
    particles. Repeat the uniform sampling step $M$ times and send these
    sampled sub-groups to $M$ different compute nodes.
  - Resample $N_m$ particles in parallel from sub-population at
    each compute node.

In standard resampling algorithms, e.g. systematic resampling, the replication
counts $r^k$ of a particle $x^k$ is subject to the constraint
\( E[r^k]=\frac{Nw^k}{\sum_j^N{w^j}} \). This constraint is sometimes known as the
``unbiasedness'' or ``proper weighting'' condition to guarantee the resampled
particle set to be an unbiased estimation. In our proposed resampling scheme,
let $\bar{r}^{i,j}$ be replication counts of uniform sampling step for particle
$\tilde{x}^{i,j}$ and $\hat{r}^{i,j}$ be replication counts of the final
resampling step. Thus the combined resampling counts of our resampling scheme is
$\bar{r}^{i,j}\hat{r}^{i,j}$. Since we have
\begin{equation}
E[\bar{r}^{i,j}]=\frac{\sum_{i=1}^M{\bar{W}_m^i}}{\sum_{i=1}^M{\tilde{W}_m^i}}
\end{equation}
\begin{multline}
E[\hat{r}^{i,j}]=E[E[\hat{r}^{i,j}\mid \{\hat{x}^{i,(j)},\hat{w}^{i,(j)}\}
_{j=1}^{\hat{N}_m}]]\\=E[\frac{N_m\hat{w}^{i,j}}{\sum_{j=1}^{\hat{N}_m}{
\hat{w}^{i,j}}}]=\frac{N_m\tilde{w}^{i,j}}{E[\hat{W}_m]}=\frac{N_m\tilde{w}
^{i,j}}{\sum_{i=1}^M{\frac{\bar{W}_m^i}{M}}},
\end{multline}
the expected value of final resampling counts can be represented as
\begin{equation}
E[\bar{r}^{i,j}\hat{r}^{i,j}]=E[\bar{r}^{i,j}]E[\hat{r}^{i,j}]=\frac{\tilde{w}^{i,j}N}{
\sum_{i=1}^M\sum_{j=1}^{N_m^i}{\tilde{w}^{i,j}}},
\end{equation}
our resampling scheme is unbiased.

* MapReduce

#+CAPTION: MapReduce Framework
#+NAME:
#+ATTR_LATEX: :width 10cm
[[file:mrframework.eps]]

MapReduce is a programming framework for processing large data sets on clusters
of computers (nodes). In this framework, the computation takes a set of input
key/value pairs, and produces a set of output key/value pairs. The Mapper
class has a map method that is called once for each input key/value pair to
generate a set of intermediate key/value pairs, an optional setup method that
is called once before the first map call, and an optional cleanup method that
is called once after the last map call. A Mapper object is initialized for each
Map task. The MapReduce framework sorts the Map task outputs by their keys,
groups those that have the same key, and distributes them to the available
Reducers. The Reducer class has a reduce method that is called once for
records that share a common intermediate key and the same optional setup and
cleanup methods as the Mapper class. Each reduce task maintains a Reducer
instance. Keys and values can be stored in any format, provided that keys can be
compared to one another and sorted. Conceptually, map and reduce methods can
be described as
\begin{gather}
  \notag
  \text{map:} \quad (k_1,v_1) \rightarrow list(k_2,v_2)\\
  \text{reduce:} \quad (k_2,\text{list}(v_2)) \rightarrow list(v_3).
\end{gather}
~list~ denotes a list of objects, $k_1$ and $k_2$ represent key types,$v_1$ and
$v_2$ are value types. The input key/value pairs $(k_1, v_1)$ are pairwise
independent, thus, map can be invoked in parallel for all pairs, yielding an
intermediate list of mapped $(k_2, v_2)$ pairs. For each key $k_2$, the
corresponding values $v_2$ are grouped and passed to the reduce method, which
merges, or reduces, final result values to a list of type $v_3$. Programmers
only have to define map and reduce methods to specify how input data is
processed, grouped by and aggregated, the framework takes care of everything
else, including data distribution, communication, synchronization and fault
tolerance. This makes writing distributed applications with MapReduce much
easier, as the framework allows the programmers to concentrate on the algorithm
and is able to handle almost everything else. Parallelization in the MapReduce
framework is achieved by executing multiple Map and Reduce tasks concurrently on
different nodes in the cluster.For embarrassingly parallel problems, e.g., parse
a large text collection or independently analyze a large number of images, this
would be a common pattern. Due to the barrier between the Map and Reduce tasks,
the Map phase of a job is only as fast as the slowest Map task. Similarly, the
completion time of a job is bounded by the running time of the slowest Reduce
task. In MapReduce, synchronization is accomplished by a barrier between the Map
and Reduce phases of processing. In configuring a MapReduce job, the programmer
provides a hint on the number of map tasks to run, but the framework makes the
final determination based on the physical layout of the data. In contrast with
the number of map tasks, the programmer can precisely specify the number of
reduce tasks.

Input data, stored on a distributed file system, are split by MapReduce into
blocks and distributed to Map tasks for processing. Output key-value pairs from
each Reduce task are written persistently back onto the distributed file system
(whereas intermediate key-value pairs are transient and not preserved). The
output ends up in $r$ files on the distributed file system, where $r$ is the
number of Reducers. MapReduce jobs can contain no Reducers, in which case Mapper
output is directly written to distributed file system (one file per Mapper).

There are many different implementations of the MapReduce programming model,
among which Apache’s Hadoop is the most well-known one and it has been
successfully applied for file based datasets. The Hadoop project includes the
Hadoop distributed file system (HDFS), designed for storing extremely large data
files (Petabytes and up) on a distributed network of computers, and Hadoop
MapReduce, the parallel computation engine. Although Hadoop is written in Java,
developers can write jobs in any other programming language using a utility
called Hadoop Streaming. Hadoop Streaming implements map and reduce methods as
interfaces to external user-specified applications. External MapReduce
applications communicate with Hadoop Streaming through standard Unix
streams. They read input key/value pair via standard input (stdin) and write
back their output via standard output (stdout).

* Implementations

In this section, we adapt bootstrap filter under above resampling schemes to the
MapReduce framework. Each iteration of bootstrap filter is one MapReduce job
with a driver to set up the iterations. These implementations can be easily
extend to other kind of particle filtering algorithms, e.g. auxiliary particle
filter. The following implementations show that our proposed resampling scheme
for distributed bootstrap filter is not only fully distributed but also well
suited to MapReduce programming framework.

1. Centralized resampling\\
   This is the most straightforward MapReduce implementation of distributed
   bootstrap filter. Each Map task performs propagation step concurrently on a
   sub-population of particles while a single Reduce task is applied for the
   centralized resampling step. The input key/value pair for map task is particle
   id and particle value including state variable and its associated weight. Each
   Map task emits a intermediate key/value pair with key being a string
   representing the centralized Reducer and value being a list of propagated
   particles. The Reduce task performs resampling on all propagated particles
   aggregated from Map tasks.

   \begin{algorithm}
     \caption{Centralized Resampling (CR)}
     \begin{tabbing}
       \textbf{cl}\=\textbf{ass Mapper}\\
       \>\textbf{me}\=\textbf{thod setup}()\\
       \>\>Read Model\\
       \>\>Read Observation $y_n$\\
       \>\textbf{method map}(sample $j$, [particle $x_{n-1}^{i,j}$, weight $w_{n-1}^{i,j}$])\\
       \>\>Propagate $\tilde{x}_n^{i,j} \sim p(x_n \mid x_{n-1}^{i,j})$\\
       \>\>Update weight $\tilde{w}_n^{i,j} \leftarrow p(y_n \mid \tilde{x}_n^{i,j})w_{n-1}^{i,j}$\\
       \>\>Emit(`centrelizedReducer', [$\tilde{x}_n^{i,j}$, $\tilde{w}_n^{i,j}$])\\
       \textbf{class Reducer}\\
       \>\textbf{method reduce}(string $str$, list([particle $\tilde{x}_n^1$, weight $\tilde{w}_n^1$], $\ldots$))\\
       \>\>Calculate sum of weights $W \leftarrow \sum_{k=1}^N{\tilde{w}_n^k}$\\
       \>\>\textbf{fo}\=\textbf{r} $k=1$ to $N$ \textbf{do}\\
       \>\>\>Normalize $\tilde{w}_n^k \leftarrow \frac{\tilde{w}_n^k}{W}$\\
       \>\>\textbf{end for}\\
       \>\>\textbf{for} $k=1$ to $N$ \textbf{do}\\
       \>\>\>Resample $x_n^k$ from $\{\tilde{x}_n^{(k)}\}_{k=1}^N$ with probability proportional to $\{\tilde{w}_n^{(k)}\}_{k=1}^N$\\
       \>\>\>Emit($k$, [$x_n^k$, $\frac{1}{N}$])\\
       \>\>\textbf{end for}
      \end{tabbing}
   \end{algorithm}

   #+CAPTION: MapReduce Framework for centralized resampling Bootstrap filter
   #+NAME:
   #+ATTR_LATEX: :width 10cm
   [[file:cendpf.eps]]

2. Distributed resampling with centralized weight normalization\\
   This scheme implements a map-side resampling. After performing propagation
   step, each propagated particle is stored in local cache of the Map task.
   Resampling is perfomred in cleanup method for all particles in local cache of
   each Map task. Particles in the same Map task are equally weighted as
   $\frac{W_m^i}{N_m^i}$. A single Reduce task is used to normalize weights over
   the global particle population.

   \begin{algorithm}
     \caption{Distributed resampling with centralized weight normalization (DR1)}
     \begin{tabbing}
       \textbf{cl}\=\textbf{ass Mapper}\\
       \>\textbf{me}\=\textbf{thod setup}()\\
       \>\>Read Model\\
       \>\>Read Observation $y_n$\\
       \>\>$mapCache \leftarrow$ new list([particle, weight])\\
       \>\textbf{method map}(sample $j$, [particle $x_{n-1}^{i,j}$, weight $w_{n-1}^{i,j}$])\\
       \>\>Propagate $\tilde{x}_n^{i,j} \sim p(x_n \mid x_{n-1}^{i,j})$\\
       \>\>Update weight $\tilde{w}_n^{i,j} \leftarrow p(y_n \mid \tilde{x}_n^{i,j})w_{n-1}^{i,j}$\\
       \>\>Append [$\tilde{x}_n^{i,j}$, $\tilde{w}_n^{i,j}$] to $mapCache$\\
       \>\textbf{method cleanup}()\\
       \>\>Calculate local particle numbers in each map task $N_m^i \leftarrow mapCache$.size()\\
       \>\>Calculate local sum of weights in each map task $W_m^i=\sum_{j=1}^{N_m^i}{\tilde{w}_n^{i,j}}$\\
       \>\>\textbf{fo}\=\textbf{r} $j=1$ to $N_m^i$ \textbf{do}\\
       \>\>\>Normalize $\tilde{w}_n^{i,j} \leftarrow \frac{\tilde{w}_n^{i,j}}{W_m^i}$\\
       \>\>\textbf{end for}\\
       \>\>\textbf{for} $k=1$ to $N_m^i$ \textbf{do}\\
       \>\>\>Resample $x_n^{i,j}$ from $\{\tilde{x}_n^{i,(j)}\}_{j=1}^{N_m^i}$ with probability proportional to $\{\tilde{w}_n^{i,(j)}\}_{j=1}^{N_m^i}$\\
       \>\>\>Emit(`centrelizedReducer', [$x_n^{i,j}$, $\frac{W_m^i}{N_m^i}$])\\
       \>\>\textbf{end for}\\
       \textbf{class Reducer}\\
       \>\textbf{method reduce}(string $str$, list([particle $x_n^1$, weight $w_n^1$], $\ldots$))\\
       \>\>Calculate global sum of weights $W \leftarrow \sum_{k=1}^N{w_n^k}$\\
       \>\>\textbf{for} $k=1$ to $N$\textbf{do}\\
       \>\>\>Emit($k$, [$x_n^k$, $\frac{w_n^k}{W}$])\\
       \>\>\textbf{end for}
     \end{tabbing}
   \end{algorithm}

3. Distributed resampling with centralized sampling of resampling number\\
   This implementation consisits of a chain of three MapReduce jobs. The first job
   has no Reduce task. Its Map tasks perform the propagation step and write their
   output data onto HDFS. Each Map task of the second job reads the output of the
   first job as the input data, computes and emits the local sum of weight of
   particles processed in the Map tasks of the first job. The second job has a
   single Reduce task which calculates the resampling number for sub-population of
   particles in each Map task and writes the result onto HDFS. Like the first job,
   the third job only has Map tasks which perform map-side resampling by reading
   both the output of first job as input and the output of second job from
   distributed cache in HDFS. The number of Map tasks in all three jobs are equal.\\
   \begin{algorithm}
     \caption{Distributed resampling with centralized sampling of resampling number (DR2)}
     \begin{tabbing}
       \textbf{cl}\=\textbf{ass Mapper1}\\
       \>\textbf{me}\=\textbf{thod setup}()\\
       \>\>Read Model\\
       \>\>Read Observation $y_n$\\
       \>\textbf{method map}(sample $j$, [particle $x_{n-1}^{i,j}$, weight $w_{n-1}^{i,j}$])\\
       \>\>Propagate $\tilde{x}_n^{i,j} \sim p(x_n \mid x_{n-1}^{i,j})$\\
       \>\>Update weight $\tilde{w}_n^{i,j} \leftarrow p(y_n \mid \tilde{x}_n^{i,j})w_{n-1}^{i,j}$\\
       \>\>Emit($j$, [$\tilde{x}_n^{i,j}$, $\tilde{w}_n^{i,j}$])\\
       \textbf{class Mapper2}\\
       \>\textbf{method setup}()\\
       \>\>Initialize local sum of weights in each map task $W_m^i \leftarrow 0$\\
       \>\textbf{method map}(sample $j$, [particle $\tilde{x}_n^{i,j}$, weight $\tilde{w}_n^{i,j}$])\\
       \>\>Update $W_m^i \leftarrow W_m^i+\tilde{w}_n^{i,j}$\\
       \>\textbf{method cleanup}()\\
       \>\>Emit(`centrelizedReducer', $W_m^i$)\\
       \textbf{class Reducer2}\\
       \>\textbf{method reduce}(string $str$, list(weight $W_m^1$, $\dots$))\\
       \>\>\textbf{fo}\=\textbf{r} $i=1$ to $N_m$ \textbf{do}\\
       \>\>\>Normalize weight $W_m^i=\frac{W_m^i}{\sum_{i=1}^{N_m}{W_m^i}}$\\
       \>\>\textbf{end for}\\
       \>\>Calculate resampling number of particles for each map task $\{N_m^{(i)}\}_{i=1}^{N_m}$\\
       \>\>\>by drawing multinomially $N$ times with probability propotional to $\{W_m^{(i)}\}_{i=1}^{N_m}$\\
       \>\>\textbf{for} $i=1$ to $N_m$ \textbf{do}\\
       \>\>\>Emit($W_m^i$, $N_m^i$)\\
       \>\>\textbf{end for}\\
       \textbf{class Mapper3}\\
       \>\textbf{method setup}()\\
       \>\>Load distributed cache $resampleTable \leftarrow \{W_m^{(i)}:N_m^{(i)}\}_{i=1}^{N_m}$\\
       \>\>$mapCache \leftarrow$ new list([particle, weight])\\
       \>\textbf{method map}(sample $j$, [particle $\tilde{x}_n^{i,j}$, weight $\tilde{w}_n^{i,j}$])\\
       \>\>Append [$\tilde{x}_n^{i,j}$, $\tilde{w}_n^{i,j}$] to $mapCache$\\
       \>\textbf{method cleanup}()\\
       \>\>Calculate local sum of weights $W_m^i \leftarrow \sum_{j=1}^{N_l^i}{\tilde{w}_n^{i,j}}$\\
       \>\>Determine the resampling number of particles $N_m^i \leftarrow resampleTable[\text{`}W_m^i\text{'}]$\\
       \>\>\textbf{for} $j=1$ to $mapCache\text{.size()}$ \textbf{do}\\
       \>\>\>Normalize $\tilde{w}_n^{i,j} \leftarrow \frac{\tilde{w}_n^{i,j}}{W_m^i}$\\
       \>\>\textbf{end for}\\
       \>\>\textbf{for} $j=1$ to $N_m^i$ \textbf{do}\\
       \>\>\>Resample $x_n^{i,j}$ from $\{\tilde{x}_n^{i,(j)}\}_{j=1}^{N_m^i}$ with probability proportional to $\{\tilde{w}_n^{i,(j)}\}_{j=1}^{N_m^i}$\\
       \>\>\>Emit($j$, [$x_n^{i,j}$, $\frac{1}{N}$])\\
       \>\>\textbf{end for}
     \end{tabbing}
   \end{algorithm}

4. Fully distributed resampling with uniform sampling and particle regrouping\\
   This implementation has both multiple number of Map tasks and Reduce tasks.  The
   number of Reduce task is equal to the number of groups of particles in each Map
   tasks. In map method, the propagation step is performed and then each propagated
   particle is stored in local cache of Map task. The cleanup method of Map task
   first sampling $M$ groups of particles by performing uniform sampling step over
   all particles in local cache. Then $M$ intermediate key/value pairs are emitted
   where each key is a string representing one Reduce task and its associated value
   contains a group of sampled particles. Each Reduce task takes one intermediate
   key/value pair aggregated from Map tasks as its input. The reduce-side
   resampling step is then performed over all regrouping particles contained in
   that value.

   \begin{algorithm}
     \caption{Fully distributed resampling with uniform sampling and particle regrouping (FDR)}
     \begin{tabbing}
       \textbf{cl}\=\textbf{ass Mapper}\\
       \>\textbf{me}\=\textbf{thod setup}()\\
       \>\>Read Model\\
       \>\>Read Observation $y_n$\\
       \>\>$mapCache \leftarrow$ new list([particle, weight])\\
       \>\>Set number of groups equal to number of reducers $N_g \leftarrow N_{red}$\\
       \>\textbf{method map}(sample $j$, [particle $x_{n-1}^{i,j}$, weight $w_{n-1}^{i,j}$])\\
       \>\>Propagate $\tilde{x}_n^{i,j} \sim p(x_n \mid x_{n-1}^{i,j})$\\
       \>\>Update weight $\tilde{w}_n^{i,j} \leftarrow p(y_n \mid \tilde{x}_n^{i,j})w_{n-1}^{i,j}$\\
       \>\>Append [$\tilde{x}_n^{i,j}$, $\tilde{w}_n^{i,j}$] to $mapCache$\\
       \>\textbf{method cleanup}()\\
       \>\>Calculate number of particles in each group $N_s^i \leftarrow \lceil \frac{N_m^i}{N_g} \rceil$\\
       \>\>\textbf{fo}\=\textbf{r} $l=1$ to $N_g$ \textbf{do}\\
       \>\>\>\textbf{fo}\=\textbf{r} $j=1$ to $N_s^i$ \textbf{do}\\
       \>\>\>\>Draw uniformly one item [$\tilde{x}_n^{i,l,j}$, $\tilde{w}_n^{i,l,j}$] from $mapCache$\\
       \>\>\>\>Emit('Reducer'+'l', [$\tilde{x}_n^{i,l,j}$, $\tilde{w}_n^{i,l,j}$])\\
       \>\>\>\textbf{end for}\\
       \>\>\textbf{end for}\\
       \textbf{class Reducer}\\
       \>\textbf{method reduce}(string $str$, list([particle $\tilde{x}_n^{i,l,1}$, weight $\tilde{w}_n^{i,l,1}$], $\dots$))\\
       \>\>Calculate local sum of weights in each reducer task $W_r^l \leftarrow \sum_{i=1}^{N_{map}}\sum_{j=1}^{N_r^l}{\tilde{w}_n^{i,l,j}}$\\
       \>\>\textbf{fo}\=\textbf{r} $j=1$ to $N_r^l$ \textbf{do}\\
       \>\>\>Normalize $\tilde{w}_n^{i,l,j} \leftarrow \frac{\tilde{w}_n^{i,l,j}}{W_r^l}$\\
       \>\>\textbf{end for}\\
       \>\>Calculate resample number of particles for each reducer $N_r \leftarrow \frac{N}{N_{red}}$\\
       \>\>\textbf{for} $k=1$ to $N_r$ \textbf{do}\\
       \>\>\>Resample $x_n^{l,k}$ from $\{\tilde{x}_n^{(i),l,(j)}\}_{i=1,j=1}^{N_{map},N_r^l}$ with
             probability proportional to $\{\tilde{w}_n^{(i),l,(j)}\}_{i=1,j=1}^{N_{map},N_r^l}$\\
       \>\>\>Emit($k$, [$x_n^{l,k}$, $\frac{1}{N}$])\\
       \>\>\textbf{end for}
     \end{tabbing}
   \end{algorithm}

   #+CAPTION: MapReduce Framework for fully distributed Bootstrap filter
   #+NAME:
   #+ATTR_LATEX: :width 11cm
   [[file:regroupdpf.eps]]

* Simulation

To test these algorithms, we consider here a simple nonlinear time series model
which has been used extensively in the literature for benchmarking numerical
filtering techniques. The state-space equations are as follows:
\begin{align}
x_n&=\frac{x_{n-1}}{2}+25\frac{x_{n-1}}{1+x_{n-1}^2}+8\cos(1.2n)+u_n,\\
y_n&=\frac{x_n^2}{20}+v_n
\end{align}
where $u_n\sim \mathcal{N}(0,\sigma_u^2)$ and $v_n\sim \mathcal{N}(0,\sigma_v^2)$
and $\sigma_u^2=10$ and $\sigma_v^2=1$ are considered fixed and known.
$\mathcal{N}(\mu,\sigma^2)$ denotes the normal distribution with mean $\mu$ and
variance $\sigma^2$. The initial state distribution is
$x_0\sim \mathcal{N}(0,10)$. The representation in terms of densities
\begin{align}
f(x_n \mid x_{n-1})&=\mathcal{N} \left( x_n\mid \frac{x_{n-1}}{2}+25
\frac{x_{n-1}}{1+x_{n-1}^2}+8\cos(1.2n),\sigma_u^2 \right)\\
g(y_n\mid x_n)&=\mathcal{N} \left( y_n\mid \frac{x_n^2}{20},\sigma_v^2 \right).
\end{align}
The Hadoop cluster is set-up in a pseudo-distributed mode by installing Hadoop
1.2.1 on a 1.4GHz duo core computer. The Map and Reduce functions are written in
the Python language and executed using Hadoop streaming. The Bootstrap filter
combined with a systematic resampling is simulated under all of above
distributed resampling schemes for 10k particles and 500k particles
respectively. The algorithms are executed under a single node and two-nodes
MapReduce framework where single node means one Map task and one Reduce task,
and two-node means two Map tasks plus one Reduce task for CR, DR1 and DR2, two
Map tasks plus two Reduce tasks for FDR. The estimation performance are tested
for the case of 10k particles. We run our simulations for 50 time steps and
evaluate the estimation performance in term of Root Mean Square Error (RMSE).
It can be seen that estimation performances of these resampling schemes have no
obvious difference.

#+CAPTION: Estimation performance. The number of particle is 10k.
#+NAME:
#+ATTR_LATEX: :width 12cm
[[file:estimates.eps]]

#+CAPTION: RMSE using 10k particles
#+NAME:
|    CR |   DR1 |   DR2 |   FDR |
|-------+-------+-------+-------|
| 4.074 | 4.098 | 4.117 | 4.105 |

The execution times are calculated for the case of 500k particles. Since the
parallelization of MapReduce framework, the all the schemes have shorter
execution times in the case of two node. DR2 takes the longest time since it
involves multiple MapReduce jobs per iteration. The startup cost of a MapReduce
job is expensive. The single node excution times of CR, DR1 and FDR are similar,
since this is actually a serial implementation of algorithms. In the case of
two-node execution, FDR yields better speedup as it is a full parallelization
which has two Map tasks as well as two Reduce tasks while CR and DR1 can only
have one Reduce task.

#+CAPTION: Execution time per iteration. The number of particle is 500k.
#+NAME:
|         | CR   | DR1  | DR2  | FDR  |
|---------+------+------+------+------|
| 1 Node  | 164s | 162s | 182s | 163s |
|---------+------+------+------+------|
| 2 Nodes | 102s | 99s  | 121s | 96s  |

* Conclusion

In this paper, a fully distributed resampling schemes for distributed partilce
filters is proposed. we have also shown how to apply the MapReduce framework to
Bootstrap filter under different distributed resampling schemes. The proposed
resampling scheme is well suited to MapReduce programming model. The simulation
result shows that our fully distributed resampling scheme outperforms other
schemes in execution time while keep the estimation accuracy at the same level.

* Acknowledgment

\begin{thebibliography}{20}
\bibitem{ref:rd05} R. Douc, O. Cappe and E. Moulines
``Comparison of Resampling Schemes for Particle Filtering''
\emph{Proceedings of the 4th International Symposium on Image and Signal
Processing and Analysis} (2005)
\bibitem{ref:njg93} N. J. Gordon, D. J. Salmond, and A. F. Smith
``A novel approach to non-linear and non-Gaussian Bayesian state estimation,''
\emph{IEE-Proceedings} F 140(1993) 107-13.
\bibitem{ref:mkp99} M. K. Pitt and N. Shephard
``Filtering via simulation: Auxiliary particle filters,''
\emph{ Journal of the American Statistical Association} 94(1999) 590–599.
\bibitem{ref:gp11} G. Pratx and L. Xing,
``Monte Carlo simulation of photon migration in a cloud computing environment
with MapReduce,''
\emph{J.Biomed.Opt.} 16(12), 125003(2011).
\bibitem{ref:td13} T. Dalman, T. Dornemann, E. Juhnke, M. Weitzel, W. Wiechert,
K. Noh, B. Freisleben,
``Cloud MapReduce for Monte Carlo bootstrap applied to Metabolic Flux Analysis,''
\emph{Future Generation Computer Systems} 29(2013) 582-590.
\bibitem{ref:sns12} S. N. Srirama, P. Jakovits and E. Vainikko,
``Adapting scientific computing problems to clouds using MapReduce,''
\emph{Future Generation Computer Systems} 28(2012) 184-192.
\bibitem{ref:bw10} B. White, T. Yeh, J. Lin, and L. Davis,
``Web-ScaleComputer Vision using MapReduce Multimedia Data Mining for,''
In \emph{MDMKDD} 10(2010).
\bibitem{ref:hb07} H. A. P. Blom and E. A. Bloem,
``Exact Bayesian and Particle Filtering of Stochastic Hybrid Systems,''
\emph{IEEE Transactions on Aerospace and Electronic Systems} 43(2007) 55-70.
\bibitem{ref:cmc10} C. M. Carvalho, M. S. Johannes, H. F. Lopes and N. G. Polson,
``Particle Learning and Smoothing,''
\emph{Statistical Science} 25(2010) 88–106.
\bibitem{ref:ad08} A. Doucet and A. M. Johansen,
``A Tutorial on Particle Filtering and Smoothing: Fifteen years later,''
In \emph{Handbook of Nonlinear Filtering} (2009).
\bibitem{ref:njg93} N. J. Gordon, D. J. Salmond and A.F.M. Smith,
``Novel approach to nonlinear/non-Gaussian Bayesian state estimation,''
\emph{IEE-Proceedings-F} 140(1993) 107–113.
\bibitem{ref:mkp01} M. K. Pitt, and N. Shephard,
``Auxiliary variable based particle filters,''
In \emph{Sequential Monte Carlo Methods in Practice} (2001).
\bibitem{ref:jl01} J. Liu and M. West,
``Combined parameters and state estimation in simulation based filtering,''
In \emph{Sequential Monte Carlo Methods in Practice} (2001).
\bibitem{ref:cv13} C. Verg, C. Dubarry, P. D. Moral and E, Moulines
``On parallel implementation of Sequential Monte Carlo methods:
the island particle model,''
In \emph{Statistics and Computing} (2013).
\bibitem{ref:mb05} M. Bolic, P. M. Djuric, and S. Hong,
``Resampling Algorithms and Architectures for Distributed Particle Filters,''
\emph{IEEE Transactions on signal processing}, 53(2005) 2442-2450.
\bibitem{ref:lmm13} L. M. Murray, A. Lee and P. E. Jacob
``Rethinking resampling in the particle filter on graphics processing units''
\bibitem{ref:pg2012} P. Gong, Y. O. Basciftci, and F. Ozguner
``A Parallel Resampling Algorithm for Particle Filtering on Shared-Memory
Architectures''
In \emph{IEEE International Parallel and Distributed Processing Symposium
Workshops} (2012)
\bibitem{ref:acs08} A. C. Sankaranarayanan, A. Srivastava, and R. Chellappa,
``Algorithmic and Architectural Optimizations for Computationally Efficient
Particle Filtering''
\emph{IEEE Transactions on image processing} 17(2008)
}

\end{thebibliography}
